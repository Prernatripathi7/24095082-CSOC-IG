{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN/zi9k5miHH+L0vBmUt1yX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prernatripathi7/24095082-CSOC-IG/blob/main/Week%201/Vanilla_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atsnGP1w_b5T",
        "outputId": "9c837d8e-735e-4201-9bc7-91ffcdad790b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.6620\n",
            "Epoch 100, Loss: 0.5724\n",
            "Epoch 200, Loss: 0.5561\n",
            "Epoch 300, Loss: 0.5423\n",
            "Epoch 400, Loss: 0.5310\n",
            "Epoch 500, Loss: 0.5222\n",
            "Epoch 600, Loss: 0.5162\n",
            "Epoch 700, Loss: 0.5104\n",
            "Epoch 800, Loss: 0.5054\n",
            "Epoch 900, Loss: 0.5025\n",
            "Epoch 1000, Loss: 0.4985\n",
            "Epoch 1100, Loss: 0.4949\n",
            "Epoch 1200, Loss: 0.4922\n",
            "Epoch 1300, Loss: 0.4900\n",
            "Epoch 1400, Loss: 0.4887\n",
            "Epoch 1500, Loss: 0.4865\n",
            "Epoch 1600, Loss: 0.4847\n",
            "Epoch 1700, Loss: 0.4842\n",
            "Epoch 1800, Loss: 0.4814\n",
            "Epoch 1900, Loss: 0.4822\n",
            "Epoch 2000, Loss: 0.4798\n",
            "Epoch 2100, Loss: 0.4792\n",
            "Epoch 2200, Loss: 0.4788\n",
            "Epoch 2300, Loss: 0.4772\n",
            "Epoch 2400, Loss: 0.4763\n",
            "Epoch 2500, Loss: 0.4751\n",
            "Epoch 2600, Loss: 0.4748\n",
            "Convergence Time: 6145.7082\n",
            "Precision-Recall AUC: 0.3295\n",
            "Confusion Matrix:\n",
            "[[9177 8492]\n",
            " [1003 3434]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Show       0.90      0.52      0.66     17669\n",
            "     No-show       0.29      0.77      0.42      4437\n",
            "\n",
            "    accuracy                           0.57     22106\n",
            "   macro avg       0.59      0.65      0.54     22106\n",
            "weighted avg       0.78      0.57      0.61     22106\n",
            "\n",
            "Max CUDA Memory Allocated: 17.63 MB\n",
            "Max CUDA Memory Reserved: 22.00 MB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, precision_recall_curve, auc, confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "df = pd.read_csv(\"KaggleV2-May-2016.csv\").dropna()\n",
        "df = pd.concat([df, pd.get_dummies(df['Gender'], prefix='Gender')], axis=1)\n",
        "df = pd.concat([df, pd.get_dummies(df['Neighbourhood'], prefix='Neighbourhood')], axis=1)\n",
        "df['No-show'] = df['No-show'].map({'No': 0, 'Yes': 1})\n",
        "df['ScheduledDay'] = pd.to_datetime(df['ScheduledDay'])\n",
        "df['AppointmentDay'] = pd.to_datetime(df['AppointmentDay'])\n",
        "df['ScheduledDate'] = df['ScheduledDay'].dt.date\n",
        "df['AppointmentDate'] = df['AppointmentDay'].dt.date\n",
        "df['WaitingDays'] = (df['AppointmentDate'] - df['ScheduledDate']).apply(lambda x: x.days)\n",
        "\n",
        "X_df = df.drop(columns=[\n",
        "    'No-show', 'ScheduledDay', 'AppointmentDay', 'ScheduledDate', 'AppointmentDate',\n",
        "    'PatientId', 'AppointmentID', 'Neighbourhood', 'Gender'\n",
        "])\n",
        "for col in X_df.columns:\n",
        "    if X_df[col].nunique() > 2:\n",
        "        std = X_df[col].std()\n",
        "        if std != 0:\n",
        "            X_df[col] = (X_df[col] - X_df[col].mean()) / std\n",
        "X = X_df.values.astype(np.float32)\n",
        "y = df['No-show'].values.astype(np.int64)\n",
        "\n",
        "trainx, valx, trainy, valy = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(trainy), y=trainy)\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "\n",
        "batch_size = 128\n",
        "train_dataset = TensorDataset(torch.tensor(trainx), torch.tensor(trainy))\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,num_workers=2,pin_memory=True)\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 80)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(80, 40)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.out = nn.Linear(40, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu1(self.fc1(x))\n",
        "        x = self.relu2(self.fc2(x))\n",
        "        return self.out(x)\n",
        "\n",
        "model = Net(trainx.shape[1]).to(device)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.05)\n",
        "\n",
        "epochs = 2700\n",
        "losses = []\n",
        "start_time=time.time()\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    losses.append(epoch_loss)\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {epoch_loss:.4f}\")\n",
        "end_time=time.time()\n",
        "max_memory_allocated = torch.cuda.max_memory_allocated(device) / (1024 ** 2)\n",
        "max_memory_reserved = torch.cuda.max_memory_reserved(device) / (1024 ** 2)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    val_inputs = torch.tensor(valx).to(device)\n",
        "    logits = model(val_inputs)\n",
        "    probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "    preds_class1 = probs[:, 1]\n",
        "\n",
        "best_threshold = 0.35\n",
        "val_preds_thresh = (preds_class1 > best_threshold).astype(int)\n",
        "convergence_time=end_time-start_time\n",
        "pr, rc, _ = precision_recall_curve(valy, preds_class1)\n",
        "pr_auc = auc(rc, pr)\n",
        "cm = confusion_matrix(valy, val_preds_thresh)\n",
        "print(f\"Convergence Time: {convergence_time:.4f}\")\n",
        "print(f\"Precision-Recall AUC: {pr_auc:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "report = classification_report(valy, val_preds_thresh, target_names=['Show', 'No-show'])\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n",
        "print(f\"Max CUDA Memory Allocated: {max_memory_allocated:.2f} MB\")\n",
        "print(f\"Max CUDA Memory Reserved: {max_memory_reserved:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import psutil\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_recall_curve, auc, confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "df = pd.read_csv(\"KaggleV2-May-2016.csv\")\n",
        "df = df.dropna(axis=0)\n",
        "\n",
        "gender_encoded = pd.get_dummies(df['Gender'], prefix='Gender')\n",
        "neighbourhood_encoded = pd.get_dummies(df['Neighbourhood'], prefix='Neighbourhood')\n",
        "df = pd.concat([df, gender_encoded,neighbourhood_encoded], axis=1)\n",
        "df = df.drop(['Gender'], axis=1)\n",
        "df['No-show'] = df['No-show'].map({'No': 0, 'Yes': 1})\n",
        "y = pd.get_dummies(df['No-show']).values.astype(np.float32)\n",
        "df['ScheduledDay'] = pd.to_datetime(df['ScheduledDay'])\n",
        "df['AppointmentDay'] = pd.to_datetime(df['AppointmentDay'])\n",
        "df['ScheduledDate'] = df['ScheduledDay'].dt.date\n",
        "df['AppointmentDate'] = df['AppointmentDay'].dt.date\n",
        "df['WaitingDays'] = (df['AppointmentDate'] - df['ScheduledDate']).apply(lambda x: x.days)\n",
        "X_df = df.drop(columns=[\n",
        "    'No-show', 'ScheduledDay', 'AppointmentDay', 'ScheduledDate', 'AppointmentDate',\n",
        "    'PatientId', 'AppointmentID','Neighbourhood'\n",
        "])\n",
        "\n",
        "for col in X_df.columns:\n",
        "    if X_df[col].nunique() > 2:\n",
        "        std = X_df[col].std()\n",
        "        if std == 0:\n",
        "            continue\n",
        "        X_df[col] = (X_df[col] - X_df[col].mean()) / std\n",
        "\n",
        "X = X_df.values.astype(np.float32)\n",
        "trainx, valx, trainy, valy = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1.0, 0.0)\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "np.random.seed(2)\n",
        "layer1_nodes = 64\n",
        "layer2_nodes = 32\n",
        "output_nodes = 2\n",
        "\n",
        "weights1 = np.random.randn(trainx.shape[1], layer1_nodes) * np.sqrt(1 / trainx.shape[1])\n",
        "weights2 = np.random.randn(layer1_nodes, layer2_nodes) * np.sqrt(1 / layer1_nodes)\n",
        "weights3 = np.random.randn(layer2_nodes, output_nodes) * np.sqrt(1 / layer2_nodes)\n",
        "\n",
        "biases1 = np.zeros((1, layer1_nodes))\n",
        "biases2 = np.zeros((1, layer2_nodes))\n",
        "biases3 = np.zeros((1, output_nodes))\n",
        "def feed_forward(X, W1, b1, W2, b2, W3, b3):\n",
        "    Z1 = np.dot(X, W1) + b1\n",
        "    A1 = relu(Z1)\n",
        "    Z2 = np.dot(A1, W2) + b2\n",
        "    A2 = relu(Z2)\n",
        "    Z3 = np.dot(A2, W3) + b3\n",
        "    A3 = softmax(Z3)\n",
        "    return Z1, A1, Z2, A2, Z3, A3\n",
        "def backpropagation(X, y, Z1, A1, Z2, A2, Z3, A3, W2, W3, class_weights):\n",
        "    m = X.shape[0]\n",
        "    weights_array = np.array([class_weights[0], class_weights[1]])\n",
        "    sample_weights = np.sum(y * weights_array, axis=1)[:, np.newaxis]\n",
        "\n",
        "    dZ3 = (A3 - y) * sample_weights\n",
        "    dW3 = np.dot(A2.T, dZ3) / m\n",
        "    db3 = np.sum(dZ3, axis=0, keepdims=True) / m\n",
        "\n",
        "    dA2 = np.dot(dZ3, W3.T)\n",
        "    dZ2 = dA2 * relu_derivative(Z2)\n",
        "    dW2 = np.dot(A1.T, dZ2) / m\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "\n",
        "    dA1 = np.dot(dZ2, W2.T)\n",
        "    dZ1 = dA1 * relu_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1) / m\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "\n",
        "    return dW1, db1, dW2, db2, dW3, db3\n",
        "def balanced_cross_entropy(y_true, y_pred, class_weights):\n",
        "    epsilon = 1e-8\n",
        "    weights_array = np.array([class_weights[0], class_weights[1]])\n",
        "    sample_weights = np.sum(y_true * weights_array, axis=1)\n",
        "    loss = -np.sum(y_true * np.log(y_pred + epsilon), axis=1)\n",
        "    weighted_loss = sample_weights * loss\n",
        "\n",
        "    return np.mean(weighted_loss)\n",
        "\n",
        "def get_batches(X, y, batch_size):\n",
        "    indices = np.arange(X.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    for start in range(0, X.shape[0], batch_size):\n",
        "        end = start + batch_size\n",
        "        batch_idx = indices[start:end]\n",
        "        yield X[batch_idx], y[batch_idx]\n",
        "\n",
        "trainy_labels = np.argmax(trainy, axis=1)\n",
        "class_weights_array = compute_class_weight(class_weight='balanced', classes=np.array([0, 1]), y=trainy_labels)\n",
        "class_weights = {0: class_weights_array[0], 1: class_weights_array[1]}\n",
        "learning_rate = 0.05\n",
        "epochs = 2700\n",
        "batch_size = 128\n",
        "loss_history = []\n",
        "start_time=time.time()\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    for X_batch, y_batch in get_batches(trainx, trainy, batch_size):\n",
        "        Z1, A1, Z2, A2, Z3, A3 = feed_forward(X_batch, weights1, biases1, weights2, biases2, weights3, biases3)\n",
        "        loss = balanced_cross_entropy(y_batch, A3, class_weights)\n",
        "        epoch_loss += loss * X_batch.shape[0]\n",
        "        dW1, db1, dW2, db2, dW3, db3 = backpropagation(X_batch, y_batch, Z1, A1, Z2, A2, Z3, A3, weights2, weights3, class_weights)\n",
        "\n",
        "        weights1 -= learning_rate * dW1\n",
        "        biases1 -= learning_rate * db1\n",
        "        weights2 -= learning_rate * dW2\n",
        "        biases2 -= learning_rate * db2\n",
        "        weights3 -= learning_rate * dW3\n",
        "        biases3 -= learning_rate * db3\n",
        "\n",
        "    epoch_loss /= trainx.shape[0]\n",
        "    loss_history.append(epoch_loss)\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {epoch_loss:.4f}\")\n",
        "end_time=time.time()\n",
        "process = psutil.Process(os.getpid())\n",
        "mem_info = process.memory_info()\n",
        "rss_memory = mem_info.rss / (1024 ** 2)\n",
        "vms_memory = mem_info.vms / (1024 ** 2)\n",
        "def predict(X, W1, b1, W2, b2, W3, b3):\n",
        "    _, _, _, _, _, A3 = feed_forward(X, W1, b1, W2, b2, W3, b3)\n",
        "    preds = np.argmax(A3, axis=1)\n",
        "    return preds, A3\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    precision_recall_curve,\n",
        "    auc,\n",
        "    confusion_matrix\n",
        ")\n",
        "val_preds_binary, val_probs = predict(valx, weights1, biases1, weights2, biases2, weights3, biases3)\n",
        "\n",
        "valy_flat = np.argmax(valy, axis=1)\n",
        "val_probs_class1 = val_probs[:, 1]\n",
        "\n",
        "best_threshold = 0.35\n",
        "val_preds_thresholded = (val_probs_class1 > best_threshold).astype(int)\n",
        "convergence_time=end_time-start_time\n",
        "precision, recall, _ = precision_recall_curve(valy_flat, val_probs_class1)\n",
        "pr_auc = auc(recall, precision)\n",
        "cm = confusion_matrix(valy_flat, val_preds_thresholded)\n",
        "print(f\"Convergence Time: {convergence_time:.4f}\")\n",
        "print(f\"Precision-Recall AUC: {pr_auc:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(valy_flat, val_preds_thresholded, target_names=[\"No-show=0\", \"No-show=1\"]))\n",
        "print(f\"RSS Memory Usage: {rss_memory:.2f} MB\")\n",
        "print(f\"VMS Memory Usage: {vms_memory:.2f} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcbTU724lpfw",
        "outputId": "c17ae2d7-2e07-4e49-f74d-a7b757d17382"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.6550\n",
            "Epoch 100, Loss: 0.5683\n",
            "Epoch 200, Loss: 0.5520\n",
            "Epoch 300, Loss: 0.5413\n",
            "Epoch 400, Loss: 0.5325\n",
            "Epoch 500, Loss: 0.5260\n",
            "Epoch 600, Loss: 0.5226\n",
            "Epoch 700, Loss: 0.5194\n",
            "Epoch 800, Loss: 0.5159\n",
            "Epoch 900, Loss: 0.5139\n",
            "Epoch 1000, Loss: 0.5115\n",
            "Epoch 1100, Loss: 0.5097\n",
            "Epoch 1200, Loss: 0.5084\n",
            "Epoch 1300, Loss: 0.5065\n",
            "Epoch 1400, Loss: 0.5063\n",
            "Epoch 1500, Loss: 0.5051\n",
            "Epoch 1600, Loss: 0.5037\n",
            "Epoch 1700, Loss: 0.5018\n",
            "Epoch 1800, Loss: 0.5022\n",
            "Epoch 1900, Loss: 0.5016\n",
            "Epoch 2000, Loss: 0.5005\n",
            "Epoch 2100, Loss: 0.4993\n",
            "Epoch 2200, Loss: 0.4995\n",
            "Epoch 2300, Loss: 0.4989\n",
            "Epoch 2400, Loss: 0.4988\n",
            "Epoch 2500, Loss: 0.4984\n",
            "Epoch 2600, Loss: 0.4976\n",
            "Convergence Time: 1041.6400\n",
            "Precision-Recall AUC: 0.3332\n",
            "Confusion Matrix:\n",
            "[[8253 9472]\n",
            " [ 731 3650]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   No-show=0       0.92      0.47      0.62     17725\n",
            "   No-show=1       0.28      0.83      0.42      4381\n",
            "\n",
            "    accuracy                           0.54     22106\n",
            "   macro avg       0.60      0.65      0.52     22106\n",
            "weighted avg       0.79      0.54      0.58     22106\n",
            "\n",
            "RSS Memory Usage: 1285.28 MB\n",
            "VMS Memory Usage: 10334.00 MB\n"
          ]
        }
      ]
    }
  ]
}